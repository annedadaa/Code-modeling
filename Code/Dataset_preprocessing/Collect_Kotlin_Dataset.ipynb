{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Collection"
      ],
      "metadata": {
        "id": "Wx4n87iRhltT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset collection code part consists of two classes:  \n",
        "\n",
        "*   **GitHubRepo** class aims to initialize the github repository and retrieve its content using requests library\n",
        "*   **KotlinDatasetBuilder** class allows one to walk through the files in the repository and extract the code in Kotlin\n",
        "\n",
        "P.S.: when running the code, do not forget to insert your github token.\n"
      ],
      "metadata": {
        "id": "R3tx1Fdmn3dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "from typing import Any, Dict, Optional, List"
      ],
      "metadata": {
        "id": "c8JSjZY8ldPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GitHubRepo:\n",
        "\n",
        "    def __init__(self, owner: str, repo: str, token: str = '') -> None:\n",
        "\n",
        "        self.base_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
        "        self.session = requests.Session()\n",
        "        if token:\n",
        "            self.session.headers.update({'Authorization': f'token {token}'})\n",
        "\n",
        "\n",
        "    def get_contents(self, path: str = '') -> Any:\n",
        "\n",
        "        \"\"\" Retrieve the contents of a directory in a repository \"\"\"\n",
        "\n",
        "        url = f\"{self.base_url}/contents/{path}\"\n",
        "        response = self.session.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "    def download_file(self, file_url: str) -> str:\n",
        "\n",
        "        \"\"\" Download a single file from GitHub \"\"\"\n",
        "\n",
        "        response = self.session.get(file_url)\n",
        "        response.raise_for_status()\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "m1fSzJ-AiPNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KotlinDatasetBuilder:\n",
        "\n",
        "    def __init__(self, github_repo: GitHubRepo) -> None:\n",
        "        self.repo = github_repo\n",
        "        self.dataset = []\n",
        "\n",
        "\n",
        "    def explore_and_extract(self, path: str = '') -> None:\n",
        "\n",
        "        \"\"\" Recursively explore given repository path and extract Kotlin files \"\"\"\n",
        "\n",
        "        contents = self.repo.get_contents(path)\n",
        "        for content in contents:\n",
        "            if content['type'] == 'dir':\n",
        "                try:\n",
        "                  self.explore_and_extract(content['path'])\n",
        "                except:\n",
        "                   pass\n",
        "            elif content['name'].endswith('.kt') or content['name'].endswith('.kts'):\n",
        "                file_content = self.repo.download_file(content['download_url'])\n",
        "                self.dataset.append({'path': content['path'],\n",
        "                                     'content': file_content})\n",
        "\n",
        "\n",
        "    def save_dataset(self, filename: str = 'kotlin_code_dataset.json') -> None:\n",
        "\n",
        "        \"\"\" Save the collected Kotlin code data to a JSON file \"\"\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.dataset, f, indent=4)"
      ],
      "metadata": {
        "id": "u0Y58hzejw_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert your GitHub token below to run the code"
      ],
      "metadata": {
        "id": "LL0cC9n6Afkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the GitHub repository handler\n",
        "github_repo = GitHubRepo(owner='Kotlin', repo='kotlinx.coroutines',\n",
        "                         token='YOUR_GITHUB_TOKEN')\n",
        "\n",
        "# Initialize the dataset builder\n",
        "dataset_builder = KotlinDatasetBuilder(github_repo)\n",
        "\n",
        "# Explore the repository and build the dataset\n",
        "dataset_builder.explore_and_extract()\n",
        "\n",
        "# Save the dataset to a file\n",
        "dataset_builder.save_dataset('kotlin_code_dataset.json')"
      ],
      "metadata": {
        "id": "FuATe1zEkFia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Dataset"
      ],
      "metadata": {
        "id": "9E0evDNm_MD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing of the gathered dataset adapted for Kotlin language, and preparing it for training/testing in the same format as described in the [CodeXGLUE dataset page](https://github.com/microsoft/CodeXGLUE/blob/main/Code-Code/CodeCompletion-token/dataset/py150/preprocess.py) for the Code Completion (line level) task. Token-level preprocessing is used in the beginning because in the line-level completion task the same data format was used. I follow the author's annotations for reproducibility."
      ],
      "metadata": {
        "id": "B3fQKNanBVN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_string(token: str, literals: Dict[str, List[str]]) -> str:\n",
        "\n",
        "    \"\"\" Processing string literals with predefined replacements \"\"\"\n",
        "\n",
        "    str_lit = re.sub(r'^\"(.*)\"$', r'\\1', token)  # Remove surrounding quotes\n",
        "    if str_lit in literals['str']:\n",
        "        return f\"<STR_LIT:{str_lit}>\"\n",
        "    return \"<STR_LIT>\"\n",
        "\n",
        "\n",
        "def process_number(token: str, literals: Dict[str, List[str]]) -> str:\n",
        "\n",
        "    \"\"\" Processing number literals with predefined markers \"\"\"\n",
        "\n",
        "    if token in literals['num']:\n",
        "        return f\"<NUM_LIT:{token}>\"\n",
        "    return \"<NUM_LIT>\"\n",
        "\n",
        "\n",
        "def tokenize_kotlin(code: str, literals: Dict[str, List[str]]) -> List[str]:\n",
        "\n",
        "    \"\"\" Regular expressions to identify strings and numbers \"\"\"\n",
        "\n",
        "    tokens = []\n",
        "    regex = re.compile(r'\\\".*?\\\"|\\d+\\.\\d+|\\d+')  # Simple regex to capture quoted strings and numbers\n",
        "\n",
        "    start = 0\n",
        "    for match in regex.finditer(code):\n",
        "        # Split the text before the match while preserving lines\n",
        "        before = code[start:match.start()]\n",
        "        # Preserve new lines by splitting on them and reinserting them into the token list\n",
        "        tokens.extend([x for x in re.split(r'(\\n)', before) if x])\n",
        "        token = match.group(0)\n",
        "        if token.startswith('\"'):\n",
        "            tokens.append(process_string(token, literals))\n",
        "        elif re.match(r'\\d', token):\n",
        "            tokens.append(process_number(token, literals))\n",
        "        start = match.end()\n",
        "\n",
        "    # Append remaining parts of the code that do not match the regex, preserving newlines\n",
        "    remaining_text = code[start:]\n",
        "    tokens.extend([x for x in re.split(r'(\\n)', remaining_text) if x])\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "U4W5Th9MrkWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "literals = json.load(open(\"literals.json\"))\n",
        "json_file = \"kotlin_code_dataset.json\"\n",
        "\n",
        "\n",
        "with open(json_file, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    output = []\n",
        "    for entry in data:\n",
        "        kotlin_code = entry['content']\n",
        "        tokens = tokenize_kotlin(kotlin_code, literals)\n",
        "        processed_code = \" \".join(tokens)\n",
        "        output.append(processed_code)\n",
        "\n",
        "# Write the output to a text file\n",
        "with open('kotlin_code_dataset_processed_tokens.json', 'w') as out_file:\n",
        "    content_list = [{'content': line} for line in output]\n",
        "    json.dump(content_list, out_file, indent=4)"
      ],
      "metadata": {
        "id": "9tBChKqZtUro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we will work with the Code completion task on a line level, we need to split lines as described in the [CodeXGLUE repository](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/CodeCompletion-line). The proportions of tokens in input and output are not kept because Kotlin code is different from Python code (in length as well).\n",
        "\n",
        "I removed the last 2 lines in each Kotlin code because usually it's an \"}\" symbol followed by an empty line and we are not very interested in predicting them. Instead, I took the last 1 line (after the deletion of the 2 last lines) as the one that needs to be predicted, the rest goes to the input."
      ],
      "metadata": {
        "id": "Id7KwMTSuzj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeCompletionProcessor:\n",
        "    def __init__(self, input_file: str, output_file: str) -> None:\n",
        "\n",
        "        \"\"\" Initialize the processor with file paths \"\"\"\n",
        "\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize_code(code: str) -> str:\n",
        "\n",
        "        \"\"\" Tokenize the Kotlin code by replacing new lines with a special token \"\"\"\n",
        "\n",
        "        return code.replace(\"\\n\", \" <EOL> \")\n",
        "\n",
        "\n",
        "    def process_data(self) -> None:\n",
        "\n",
        "        \"\"\" Process the Kotlin code data by splitting content such that the last line is the ground truth \"\"\"\n",
        "\n",
        "        with open(self.input_file, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        processed_data = []\n",
        "\n",
        "        for entry in data:\n",
        "            content = entry['content']\n",
        "            tokens = content.split('\\n')[:-2]\n",
        "            if len(tokens) > 1:\n",
        "                # Take all but the last line for input, and the last line for ground truth\n",
        "                input_section = \"<s> \" + self.tokenize_code('\\n'.join(tokens[:-1]))\n",
        "                gt_section = self.tokenize_code(tokens[-1])\n",
        "\n",
        "                processed_data.append({\n",
        "                    \"input\": input_section,\n",
        "                    \"gt\": gt_section\n",
        "                })\n",
        "            else:\n",
        "                # Handle the case for files with only one line\n",
        "                print(f\"Skipping file with insufficient lines: {len(tokens)} lines found.\")\n",
        "\n",
        "        with open(self.output_file, 'w') as file:\n",
        "            for item in processed_data:\n",
        "                json.dump(item, file)\n",
        "                file.write('\\n')"
      ],
      "metadata": {
        "id": "yqbaY7Eu_Noo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = CodeCompletionProcessor('/content/kotlin_code_dataset_processed_tokens.json',\n",
        "                                    '/content/kotlin_code_dataset_processed_lines.json')\n",
        "processor.process_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Qlagzr_55J",
        "outputId": "6d70bad2-2e07-417a-f85c-1c3ba966fee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping file with insufficient lines: 0 lines found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into train and test sets"
      ],
      "metadata": {
        "id": "Hlg3oxtFJvLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In total, I have 1049 samples in Kotlin. I split the dataset as follows: 949 samples for training set, 100 last samples for test set (the same amount of test data was used in the CodeXGLUE Python dataset)."
      ],
      "metadata": {
        "id": "kNKPXVpiAwCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_json(input_file: str,\n",
        "                          output_train_file: str,\n",
        "                          output_test_file: str,\n",
        "                          output_test_answers: str) -> None:\n",
        "\n",
        "    \"\"\" Load JSON objects from a file, split them into train and test sets,\n",
        "    while keeping the answers for the test set in a separate file, save all \"\"\"\n",
        "\n",
        "    data = []\n",
        "    with open(input_file, 'r') as file:\n",
        "        for line in file:\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "                continue\n",
        "\n",
        "    if len(data) < 100:\n",
        "        raise ValueError(\"The input file does not contain enough entries (100 required).\")\n",
        "\n",
        "    train_data = data[:-100]\n",
        "\n",
        "    test_data = data[-100:]\n",
        "    test_answers = []\n",
        "    for entry in test_data:\n",
        "        test_answers.append({\"gt\":entry['gt']})\n",
        "        entry['gt'] = \"\"  # Replace 'gt' values with empty strings\n",
        "\n",
        "    with open(output_train_file, 'w') as file:\n",
        "        for entry in train_data:\n",
        "            json.dump(entry, file)\n",
        "            file.write('\\n')\n",
        "\n",
        "    with open(output_test_file, 'w') as file:\n",
        "        for entry in test_data:\n",
        "            json.dump(entry, file)\n",
        "            file.write('\\n')\n",
        "\n",
        "    with open(output_test_answers, 'w') as file:\n",
        "        for entry in test_answers:\n",
        "            json.dump(entry, file)\n",
        "            file.write('\\n')"
      ],
      "metadata": {
        "id": "aH11qlzkJzBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_process_json(input_file=\"kotlin_code_dataset_processed_lines.json\",\n",
        "                      output_train_file=\"kotlin_code_train.json\",\n",
        "                      output_test_file=\"kotlin_code_test.json\",\n",
        "                      output_test_answers=\"kotlin_code_answers.json\")"
      ],
      "metadata": {
        "id": "TWK4yFU4J9Xd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}