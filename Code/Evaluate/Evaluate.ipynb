{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the [phi-1.5](https://huggingface.co/microsoft/phi-1_5) model on the Kotlin and Python test sets\n"
      ],
      "metadata": {
        "id": "aQir5H1m7Pc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to predict the next code line given the predeceding 5 lines (model cannot handle a very long sequence at one time)\n",
        "\n",
        "Another approach is to predict line by line but here I want to predict only the last one line."
      ],
      "metadata": {
        "id": "poDfoKqx9_Xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lQhrxCL52Qj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from fuzzywuzzy import fuzz\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_special_tokens(path: str) -> List[str]:\n",
        "\n",
        "    \"\"\"\n",
        "    Load special tokens from a JSON file and format them into a list.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(path, \"r\") as file:\n",
        "        literals = json.load(file)\n",
        "    tokens = [\"<STR_LIT>\", \"<NUM_LIT>\", \"<CHAR_LIT>\"]\n",
        "    tokens.extend(f\"<STR_LIT:{lit}>\" for lit in literals[\"str\"])\n",
        "    tokens.extend(f\"<NUM_LIT:{lit}>\" for lit in literals[\"num\"])\n",
        "    tokens.extend(f\"<CHAR_LIT:{lit}>\" for lit in literals[\"char\"])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def load_model(model_name: str,\n",
        "               special_tokens_path: str) -> Tuple[AutoTokenizer, nn.Module]:\n",
        "\n",
        "    \"\"\"\n",
        "    Load a pretrained tokenizer and model from Hugging Face, and add special tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    special_tokens = load_special_tokens(special_tokens_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                              additional_special_tokens=special_tokens)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.resize_token_embeddings(len(tokenizer))  # Important to resize model token embeddings\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def predict_next_line(code: str, tokenizer: AutoTokenizer,\n",
        "                      model: nn.Module, device: str = 'cuda') -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Predict the next line of code given an input sequence of code.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    inputs = tokenizer.encode(code, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs, max_length=512, num_return_sequences=1)\n",
        "    predicted_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return predicted_code\n",
        "\n",
        "\n",
        "def read_and_predict(json_file: str, tokenizer: AutoTokenizer,\n",
        "                     model: torch.nn.Module, device: str = 'cuda') -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Read JSON file containing code inputs, and predict the next line for each input.\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = []\n",
        "    with open(json_file, 'r') as file:\n",
        "        for n, line in enumerate(file):\n",
        "            try:\n",
        "                json_object = json.loads(line)\n",
        "                input_lines = json_object['input'].split('<EOL>')\n",
        "                # Keep only the last 5 lines\n",
        "                if len(input_lines) > 5:\n",
        "                    input_lines = input_lines[-5:]\n",
        "                input_code = '<EOL>'.join(input_lines) + '<EOL>'\n",
        "                num_lines = len(input_code.split('<EOL>')) - 1\n",
        "                predicted_line = predict_next_line(input_code, tokenizer, model, device)\n",
        "                predicted_line = predicted_line.replace('\\n', '<EOL>')\n",
        "                print(predicted_line.split('<EOL>'))\n",
        "                print(predicted_line.split('<EOL>')[num_lines])\n",
        "                outputs.append(predicted_line.split('<EOL>')[num_lines])\n",
        "                print(n)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error reading JSON: {e}\")\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "7mHuBEnv548S"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/phi-1.5\"\n",
        "special_tokens_path = \"literals.json\"\n",
        "tokenizer, model = load_model(model_name, special_tokens_path)"
      ],
      "metadata": {
        "id": "AfvvNdhv7iy9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage"
      ],
      "metadata": {
        "id": "hIz0gaMK9tSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_snippet = \"import numpy as np\\nnp.random.seed(42)\\n\"\n",
        "num_lines = len(code_snippet.split('\\n'))\n",
        "predicted_line = predict_next_line(code_snippet, tokenizer, model)\n",
        "print(\"Predicted next line:\", predicted_line.split('\\n')[:num_lines])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FpOOZaQ8aDI",
        "outputId": "9a240a90-7d37-4a7d-d3fc-297993c5c284"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next line: ['import numpy as np', 'np.random.seed(42)', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to evaluate the predictions"
      ],
      "metadata": {
        "id": "L8ERbUdPckaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process(code: str) -> str:\n",
        "\n",
        "    \"\"\" Converting special symbols in a code string to their respective\n",
        "    literals or removing them \"\"\"\n",
        "\n",
        "    code = code.replace(\"<NUM_LIT>\", \"0\").replace(\"<STR_LIT>\", \"\").replace(\"<CHAR_LIT>\", \"\")\n",
        "    pattern = re.compile(r\"<(STR|NUM|CHAR)_LIT:(.*?)>\", re.S)\n",
        "    lits = re.findall(pattern, code)\n",
        "    for lit in lits:\n",
        "        code = code.replace(f\"<{lit[0]}_LIT:{lit[1]}>\", lit[1])\n",
        "    return code\n",
        "\n",
        "\n",
        "def evaluate(answers_path: str, predictions_path: str) -> None:\n",
        "\n",
        "    \"\"\" Evaluating predictions against ground truth answers,\n",
        "    computing exact match (EM) and edit similarity metrics \"\"\"\n",
        "\n",
        "    data = []\n",
        "    with open(answers_path, 'r') as i_file:\n",
        "        for line in i_file:\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "                continue\n",
        "\n",
        "    with open(predictions_path, \"r\") as f:\n",
        "        gts = f.readlines()\n",
        "\n",
        "    assert len(data) == len(gts), f\"Samples of predictions and answers are not equal, {len(data)}: {len(gts)}\"\n",
        "\n",
        "    total = len(gts)\n",
        "    EM = 0.0\n",
        "    edit_sim = 0.0\n",
        "    for i, (gt, pred) in enumerate(zip(data, gts)):\n",
        "        try:\n",
        "            pred = post_process(pred.strip())\n",
        "            gt = post_process(gt[\"gt\"])\n",
        "            edit_sim += fuzz.ratio(pred, gt)\n",
        "            if pred.split() == gt.split():\n",
        "                EM += 1\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON on line {i + 1}: {gt}\")\n",
        "            print(e)\n",
        "            continue  # Skip this line or handle it as needed\n",
        "\n",
        "    edit_similarity = round(edit_sim / total, 2) if total else 0\n",
        "    exact_match = round((EM / total) * 100, 2) if total else 0\n",
        "    print(f\"Edit sim: {edit_similarity}, EM: {exact_match}\")\n"
      ],
      "metadata": {
        "id": "_eR-TdK3csMQ"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on CodeXGLUE test set (line level)"
      ],
      "metadata": {
        "id": "D9kbQlrL9wAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"/content/CodeXGLUE_test_processed.json\""
      ],
      "metadata": {
        "id": "55o7ltK4_ioh"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_outputs = read_and_predict(json_file=json_file, tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "6M_03zDo92q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('predictions_python.txt', 'w') as fp:\n",
        "    for item in python_outputs:\n",
        "        fp.write(item)\n",
        "        fp.write('\\n')"
      ],
      "metadata": {
        "id": "3jOAD0MV-Fvz"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate('/content/CodeXGLUE_test_answers.json',\n",
        "         '/content/predictions_python.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7T59O70eEMN",
        "outputId": "1f0952c1-6ea3-425a-a761-8f5c2d4514c9"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edit sim: 28.42, EM: 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Kotlin test set (line level)"
      ],
      "metadata": {
        "id": "3OvRToK0jApB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"/content/kotlin_code_test.json\""
      ],
      "metadata": {
        "id": "bPRVTvvcjFFI"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kotlin_outputs = read_and_predict(json_file=json_file, tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "0I1yRsOwjQs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('predictions_kotlin.txt', 'w') as fp:\n",
        "    for item in kotlin_outputs:\n",
        "        fp.write(item)\n",
        "        fp.write('\\n')"
      ],
      "metadata": {
        "id": "uxERiiIFj_En"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate('/content/kotlin_code_answers.json',\n",
        "         '/content/predictions_kotlin.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj_h8ZZmkCxV",
        "outputId": "2a9b4fa6-276c-4cd0-b786-6431ad720627"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edit sim: 14.69, EM: 43.0\n"
          ]
        }
      ]
    }
  ]
}